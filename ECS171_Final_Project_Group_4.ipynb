{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECS171 Final Project- Group 4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAv7KqOdQpAU"
      },
      "source": [
        "## ECS171 Final Project - Group 4\n",
        "\n",
        "**Purpose**: Customer Analysis - to identify and separate cutomers into specific segments based on various attributes (age, income, etc) using unsupervised clustering methods\n",
        "<br>**Dataset**: https://www.kaggle.com/imakash3011/customer-personality-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u448Rs1tQpAZ"
      },
      "source": [
        "#### Import needed libraries and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xk1BwRUQpAa"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install altair delayed\n",
        "!{sys.executable} -m pip install yellowbrick delayed\n",
        "!{sys.executable} -m pip install mlxtend delayed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NxgmZ6BQpAc"
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from datetime import date, datetime\n",
        "import altair as alt\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQRn61NQpAd"
      },
      "source": [
        "# Loading the dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/StephenW789/ECS-171-FinalProject/main/marketing_campaign.csv\", sep = \"\\t\")\n",
        "print(df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c75x6jGaQpAd"
      },
      "source": [
        "#### Data Cleaning and Feature Engineering\n",
        "- perform exploratory data analysis to observe the given dataset by creating some data visualizations such as graphs and a heatmap\n",
        "- remove rows with null values\n",
        "- perform feature engineering by modifying some existing attributes and creating new attributes using the given attributes, e.g. Age of the customer, Total spendings on the different products, Total number of children, etc.\n",
        "- drop redundant features and outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX6MDuiPQpAe"
      },
      "source": [
        "# print data to see how the data needs to be processed\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GSB14SR9QpAf"
      },
      "source": [
        "# DATA CLEANING\n",
        "df_clean = df.copy()\n",
        "\n",
        "# remove null values\n",
        "df_clean = df_clean.dropna()\n",
        "\n",
        "# Feature engineering\n",
        "# reference: https://www.kaggle.com/karnikakapoor/customer-segmentation-clustering/notebook#DATA-CLEANING\n",
        "# reference: https://thecleverprogrammer.com/2021/02/08/customer-personality-analysis-with-python/\n",
        "# convert \"Dt_Customer\" to a datetime object\n",
        "df['Dt_Customer'] = df['Dt_Customer'].apply(pd.to_datetime)\n",
        "# Feature for number of days that customers started to shop\n",
        "df['Customer_For'] = df['Dt_Customer'].apply(lambda x: (datetime.now() - x).days)\n",
        "df_clean['Customer_For'] = df['Customer_For']\n",
        "# print(df_clean['Customer_For'])\n",
        "\n",
        "# Feature for age of customer today \n",
        "df_clean['Age'] = datetime.now().year - df['Year_Birth']\n",
        "# print(df_clean['Age'])\n",
        "\n",
        "# Feature for total spendings on various item categories\n",
        "df_clean['Spent'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProds']\n",
        "# print(df_clean['Spent'])\n",
        "\n",
        "# Feature for simplifing marital status into 2 living situations: \"Alone\", \"Partner\"\n",
        "df_clean['Living_With'] = df_clean['Marital_Status'].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \"Absurd\":\"Alone\", \"Widow\":\"Alone\", \"YOLO\":\"Alone\", \"Divorced\":\"Alone\", \"Single\":\"Alone\",})\n",
        "# print(df_clean['Living_With'])\n",
        "\n",
        "# Feature for total children living in the household\n",
        "df_clean['Children'] = df_clean['Kidhome'] + df_clean['Teenhome']\n",
        "# print(df_clean['Children'])\n",
        "\n",
        "# Feature for total members in the household\n",
        "df_clean['Family_Size'] = df_clean['Living_With'].replace({\"Alone\": 1, \"Partner\":2}) + df_clean['Children']\n",
        "# print(df_clean['Family_Size'])\n",
        "\n",
        "# Feature for whether the customer is a parent\n",
        "df_clean['Is_Parent'] = np.where(df_clean.Children > 0, 1, 0)\n",
        "# print(df_clean['Is_Parent'])\n",
        "\n",
        "# Simplify education levels in three groups\n",
        "df_clean['Education'] = df_clean['Education'].replace({\"Basic\":\"Undergraduate\",\"2n Cycle\":\"Undergraduate\", \"Graduation\":\"Graduate\", \"Master\":\"Postgraduate\", \"PhD\":\"Postgraduate\"})\n",
        "# print(df_clean['Education'])\n",
        "\n",
        "# Rename some columns for clarity\n",
        "df_clean = df_clean.rename(columns = {\"MntWines\": \"Wines\",\"MntFruits\":\"Fruits\",\"MntMeatProducts\":\"Meat\",\"MntFishProducts\":\"Fish\",\"MntSweetProducts\":\"Sweets\",\"MntGoldProds\":\"Gold\"})\n",
        "\n",
        "#Dropping some of the redundant features\n",
        "to_drop = ['ID', 'Year_Birth', 'Marital_Status', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue']\n",
        "df_clean = df_clean.drop(to_drop, axis=1)\n",
        "print(df_clean.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vrIO_KKQpAg"
      },
      "source": [
        "# DATA VISUALIZATION\n",
        "# altair chart to check for any outliers\n",
        "some_features = ['Income', 'Recency', 'Customer_For', 'Age', 'Spent']\n",
        "alt.Chart(df_clean).mark_circle().encode(\n",
        "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
        "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
        "    color='Is_Parent:N'\n",
        ").properties(\n",
        "    width=150,\n",
        "    height=150\n",
        ").repeat(\n",
        "    row=some_features,\n",
        "    column=some_features[::-1]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-FWc79WQpAh"
      },
      "source": [
        "# remove outliers from data\n",
        "df_no_outliers = df_clean.copy()\n",
        "df_no_outliers = df_no_outliers[(df_no_outliers['Income'] < 600000)]\n",
        "# df_no_outliers = df_no_outliers[(df_no_outliers['Meat'] < 1500)]\n",
        "# df_no_outliers = df_no_outliers[(df_no_outliers['Sweets'] < 250)]\n",
        "df_no_outliers = df_no_outliers[(df_no_outliers['Age'] < 100)]\n",
        "\n",
        "df_no_outliers.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2uBO0AEQpAi"
      },
      "source": [
        "# ANOTHER DATA VISUALIZATION\n",
        "# heatmap to see correlations between the different attributes\n",
        "fig, ax = plt.subplots(figsize=(30,30))\n",
        "print(sns.heatmap(df_no_outliers.corr(), annot=True, vmin=-1, vmax=1, center=0, linewidths=.5, cmap='coolwarm', ax=ax))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyLTkJPQpAi"
      },
      "source": [
        "#### Data Preprocessing\n",
        "- encode categorical attributes\n",
        "- standardize all attributes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3VZbBNpXQpAi"
      },
      "source": [
        "# DATA PREPROCESSING\n",
        "df_encoded = df_no_outliers.copy()\n",
        "# encode all categorical attributes\n",
        "encoder = LabelEncoder()\n",
        "df_encoded['Education'] = encoder.fit_transform(np.asarray(df_encoded['Education']))\n",
        "# print(df_encoded['Education'].unique())\n",
        "df_encoded['Living_With'] = encoder.fit_transform(np.asarray(df_encoded['Living_With']))\n",
        "# print(df_encoded['Living_With'].unique())\n",
        "# print(df_encoded.head(10))\n",
        "\n",
        "# standardize all data attributes\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df_encoded.copy()\n",
        "cols_to_drop = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']\n",
        "df_scaled = df_scaled.drop(columns = cols_to_drop)\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_scaled), columns = df_scaled.columns)\n",
        "print(df_scaled.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xngAlqRGQpAj"
      },
      "source": [
        "#### Dimensionality Reduction\n",
        "- because we have a lot of attributes and the heatmap shows that some attributes have decent correlation with each other, we want to reduce the dimensionality by performing feature selection using PCA\n",
        "- create Scree Plot to determine how many principal components should be included"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gISoUzZRQpAj"
      },
      "source": [
        "# apply a temporary pca on dataset with arbitrary number of components\n",
        "# to see how many components should be included\n",
        "pca_test = PCA(n_components = 5)\n",
        "pca_test.fit(df_scaled)\n",
        "df_pca_test = pd.DataFrame(pca_test.transform(df_scaled), columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
        "\n",
        "pca_values_test = np.arange(pca_test.n_components) + 1\n",
        "plt.plot(pca_values_test, pca_test.explained_variance_, '-o', linewidth = 2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJKqtg6VQpAk"
      },
      "source": [
        "After looking at the Screen plot and doing some trial and error to determine the optimal number of principal components to include, we found that having 3 principal components gave the best distribution between clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "14F-DivHQpAk"
      },
      "source": [
        "# dimensionality reduction\n",
        "pca = PCA(n_components = 3)\n",
        "pca.fit(df_scaled)\n",
        "df_pca = pd.DataFrame(pca.transform(df_scaled), columns = ['PC1', 'PC2', 'PC3'])\n",
        "PC1, PC2, PC3 = pca.components_\n",
        "print(df_pca.head(10))\n",
        "# see how the different attributes are weighed in the principal components\n",
        "pd.set_option('display.max_columns', None)\n",
        "display(pd.DataFrame(np.vstack((PC1.reshape(1,-1), PC2.reshape(1,-1), PC3.reshape(1,-1))), columns = df_scaled.columns, index = ['PC1', 'PC2', 'PC3']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2P9FQH3QpAk"
      },
      "source": [
        "#### Method 1: Agglomerative Clustering (with results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUgTYGsNQpAl"
      },
      "source": [
        "# initiating the Agglomerative Clustering model \n",
        "# chose 4 clusters based on the optimal number of clusters for KMeans\n",
        "AC = AgglomerativeClustering(n_clusters = 4)\n",
        "# fit model and predict clusters\n",
        "df_pca_ac = df_pca.copy()\n",
        "yhat_AC = AC.fit_predict(df_pca_ac)\n",
        "df_pca_ac[\"Clusters\"] = yhat_AC\n",
        "\n",
        "alt.Chart(df_pca_ac).mark_circle().encode(\n",
        "    x=\"PC1\",\n",
        "    y=\"PC2\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0fMa9qQpAl"
      },
      "source": [
        "# evaluation of the Agglomerative Clustering results\n",
        "# looking at the distribution of the clusters\n",
        "alt.Chart(df_pca_ac).mark_bar().encode(\n",
        "    x=\"Clusters:N\",\n",
        "    y=\"count()\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjCmGslLQpAl"
      },
      "source": [
        "# adding the 'Clusters' feature to the original cleaned/encoded data\n",
        "# so we can try to profile the different clusters\n",
        "df_final_ac = df_encoded.copy()\n",
        "df_final_ac[\"Clusters\"]= yhat_AC\n",
        "\n",
        "\n",
        "alt.Chart(df_final_ac).mark_circle().encode(\n",
        "    x=\"Spent\",\n",
        "    y=\"Income\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tyRB-vtQpAm"
      },
      "source": [
        "alt.Chart(df_final_ac).mark_circle().encode(\n",
        "    x=\"Age\",\n",
        "    y=\"Spent\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgGO4V_DQpAm"
      },
      "source": [
        "alt.Chart(df_final_ac).mark_circle().encode(\n",
        "    x=\"Children\",\n",
        "    y=\"Spent\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDDDyQpQpAn"
      },
      "source": [
        "# profile the different clusters (finding similarities in each group)\n",
        "Personal = [ \"Kidhome:O\",\"Teenhome:O\",\"Customer_For:Q\", \"Age:Q\", \"Children:O\", \"Family_Size:O\", \"Is_Parent:N\", \"Education:O\",\"Living_With:O\", \"Income:Q\"]\n",
        "\n",
        "base = alt.Chart(df_final_ac).mark_bar().encode(\n",
        "    y=\"count()\",\n",
        "    color=\"Clusters:N\",\n",
        "    facet=\"Clusters:N\"\n",
        ")\n",
        "\n",
        "chart = alt.vconcat(data=df_final_ac)\n",
        "for x_encoding in Personal:\n",
        "    chart |= base.encode(x=alt.X(x_encoding, bin=True)).facet(row=\"Clusters:N\")\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF6fabR9QpAn"
      },
      "source": [
        "alt.data_transformers.disable_max_rows()\n",
        "# check to see how much of each type of product each group is consuming\n",
        "columns = list(df_final_ac.columns)\n",
        "values = [\"Wines\", \"Fruits\", \"Meat\", \"Fish\", \"Sweets\", \"Gold\"]\n",
        "alt.Chart(df_final_ac.melt(id_vars=(set(columns) - set(values)),value_vars = values)).mark_bar(size=15).encode(\n",
        "    y='median(value)',\n",
        "    x='Clusters:N',\n",
        "    color='Clusters:N',\n",
        "    facet='variable:N'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fI-tC4uQpAn"
      },
      "source": [
        "#### Method 2: Apriori Algorithm (with results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RL2p3BkQpAo"
      },
      "source": [
        "# Create Age segment\n",
        "cut_labels_Age = ['Young', 'Adult', 'Mature', 'Senior']\n",
        "cut_bins = [0, 30, 45, 65, 120]\n",
        "df_apriori = df_encoded.copy()\n",
        "df_apriori['Age_group'] = pd.cut(df_apriori['Age'], bins=cut_bins, labels=cut_labels_Age)\n",
        "#Create Income segment\n",
        "cut_labels_Income = ['Low income', 'Low to medium income', 'Medium to high income', 'High income']\n",
        "df_apriori['Income_group'] = pd.qcut(df_apriori['Income'], q=4, labels=cut_labels_Income)\n",
        "#Create Seniority segment\n",
        "cut_labels_Seniority = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\n",
        "df_apriori['Seniority_group'] = pd.qcut(df_apriori['Customer_For'], q=4, labels=cut_labels_Seniority)\n",
        "df_apriori = df_apriori.drop(columns=['Age','Income','Customer_For'])\n",
        "\n",
        "cut_labels = ['Low consumer', 'Frequent consumer', 'Biggest consumer']\n",
        "df_apriori['Wines_segment'] = pd.qcut(df_apriori['Wines'][df_apriori['Wines']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori['Fruits_segment'] = pd.qcut(df_apriori['Fruits'][df_apriori['Fruits']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori['Meat_segment'] = pd.qcut(df_apriori['Meat'][df_apriori['Meat']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori['Fish_segment'] = pd.qcut(df_apriori['Fish'][df_apriori['Fish']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori['Sweets_segment'] = pd.qcut(df_apriori['Sweets'][df_apriori['Sweets']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori['Gold_segment'] = pd.qcut(df_apriori['Gold'][df_apriori['Gold']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\n",
        "df_apriori.replace(np.nan, \"Non consumer\",inplace=True)\n",
        "df_apriori.drop(columns=['Spent','Wines','Fruits','Meat','Fish','Sweets','Gold'],inplace=True)\n",
        "df_apriori = df_apriori.astype(object)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', 999)\n",
        "pd.options.display.float_format = \"{:.3f}\".format\n",
        "association = df_apriori[[\"Age_group\", \"Seniority_group\", \"Income_group\", \"Wines_segment\", \"Fruits_segment\",\"Meat_segment\", \"Fish_segment\", \"Sweets_segment\", \"Gold_segment\"]].copy() \n",
        "df_final_apriori = pd.get_dummies(association)\n",
        "min_support = 0.08\n",
        "max_len = 10\n",
        "frequent_items = apriori(df_final_apriori, use_colnames=True, min_support=min_support, max_len=max_len + 1)\n",
        "rules = association_rules(frequent_items, metric='lift', min_threshold=1)\n",
        "\n",
        "product='Wines'\n",
        "segment='Biggest consumer'\n",
        "target = '{\\'%s_segment_%s\\'}' %(product,segment)\n",
        "results_personnal_care = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\n",
        "results_personnal_care.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTu0HyAXQpAo"
      },
      "source": [
        "#### Method 3: K-Means Clustering (with results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zcrXXbJQpAo"
      },
      "source": [
        "# K-MEANS CLUSTERING\n",
        "\n",
        "# determine the optimal number of clusters\n",
        "test_kmeans = KMeans()\n",
        "elbow_visualizer = KElbowVisualizer(test_kmeans, k = 10)\n",
        "elbow_visualizer.fit(df_pca)\n",
        "elbow_visualizer.show()\n",
        "\n",
        "# build the K-Means clustering model\n",
        "kmeans = KMeans(n_clusters=4, random_state=21)\n",
        "my_kmeans_clusters = kmeans.fit_predict(df_pca)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "df_pca_kmeans = df_pca.copy()\n",
        "df_pca_kmeans['Clusters'] = my_kmeans_clusters\n",
        "\n",
        "alt.Chart(df_pca_kmeans).mark_circle().encode(\n",
        "    x=\"PC1\",\n",
        "    y=\"PC2\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4rD3n8jQpAp"
      },
      "source": [
        "# evaluation of the K-means clustering results\n",
        "# by looking at the distribution of the clusters\n",
        "alt.Chart(df_pca_kmeans).mark_bar().encode(\n",
        "    x=\"Clusters:N\",\n",
        "    y=\"count()\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w7U7-1JQpAp"
      },
      "source": [
        "# profiling the different clusters (finding similarities in each group)\n",
        "# include the clusters in the original cleaned/encoded data so we can try to profile the different clusters\n",
        "df_final_kmeans = df_encoded.copy()\n",
        "df_final_kmeans['Clusters'] = my_kmeans_clusters\n",
        "\n",
        "alt.Chart(df_final_kmeans).mark_circle().encode(\n",
        "    x=\"Spent\",\n",
        "    y=\"Income\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh6O1-LHQpAp"
      },
      "source": [
        "alt.Chart(df_final_kmeans).mark_circle().encode(\n",
        "    x=\"Age\",\n",
        "    y=\"Spent\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSXgrgR2QpAp"
      },
      "source": [
        "alt.Chart(df_final_kmeans).mark_circle().encode(\n",
        "    x=\"Children\",\n",
        "    y=\"Spent\",\n",
        "    color=\"Clusters:N\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2EzNIHQpAp"
      },
      "source": [
        "Personal = [ \"Kidhome:O\",\"Teenhome:O\",\"Customer_For:Q\", \"Age:Q\", \"Children:O\", \"Family_Size:O\", \"Is_Parent:N\", \"Education:O\",\"Living_With:O\", \"Income:Q\"]\n",
        "\n",
        "base = alt.Chart(df_final_kmeans).mark_bar().encode(\n",
        "    y=\"count()\",\n",
        "    color=\"Clusters:N\",\n",
        "    facet=\"Clusters:N\"\n",
        ")\n",
        "\n",
        "chart = alt.vconcat(data=df_final_kmeans)\n",
        "for x_encoding in Personal:\n",
        "    chart |= base.encode(x=alt.X(x_encoding, bin=True)).facet(row=\"Clusters:N\")\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwLMle84QpAq"
      },
      "source": [
        "alt.data_transformers.disable_max_rows()\n",
        "\n",
        "columns = list(df_final_kmeans.columns)\n",
        "values = [\"Wines\", \"Fruits\", \"Meat\", \"Fish\", \"Sweets\", \"Gold\"]\n",
        "alt.Chart(df_final_kmeans.melt(id_vars=(set(columns) - set(values)),value_vars = values)).mark_bar(size=15).encode(\n",
        "    y='median(value)',\n",
        "    x='Clusters:N',\n",
        "    color='Clusters:N',\n",
        "    facet='variable:N'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtc6gLbVQpAq"
      },
      "source": [
        "#### Method 4: Fuzzy C-Means Clustering (with results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc5IGObFQpAq"
      },
      "source": [
        "np.random.seed(42)\n",
        "# online definition to draw ellipse\n",
        "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "    \"\"\"\n",
        "    Create a plot of the covariance confidence ellipse of `x` and `y`\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : array_like, shape (n, )\n",
        "        Input data.\n",
        "\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object to draw the ellipse into.\n",
        "\n",
        "    n_std : float\n",
        "        The number of standard deviations to determine the ellipse's radiuses.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.patches.Ellipse\n",
        "\n",
        "    Other parameters\n",
        "    ----------------\n",
        "    kwargs : `~matplotlib.patches.Patch` properties\n",
        "    \"\"\"\n",
        "    if x.size != y.size:\n",
        "        raise ValueError(\"x and y must be the same size\")\n",
        "\n",
        "    cov = np.cov(x, y)\n",
        "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    # Using a special case to obtain the eigenvalues of this\n",
        "    # two-dimensionl dataset.\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0),\n",
        "        width=ell_radius_x * 2,\n",
        "        height=ell_radius_y * 2,\n",
        "        facecolor=facecolor,\n",
        "        **kwargs)\n",
        "\n",
        "    # Calculating the stdandard deviation of x from\n",
        "    # the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
        "    mean_x = np.mean(x)\n",
        "\n",
        "    # calculating the stdandard deviation of y ...\n",
        "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    transf = transforms.Affine2D() \\\n",
        "        .rotate_deg(45) \\\n",
        "        .scale(scale_x, scale_y) \\\n",
        "        .translate(mean_x, mean_y)\n",
        "\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "    return ax.add_patch(ellipse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9wntwORQpAq"
      },
      "source": [
        "# compute centroids\n",
        "def compute_centroids():\n",
        "    dummy = w.transpose()\n",
        "    #x values\n",
        "    for n in range(c):\n",
        "        numerator = np.sum((dummy[n]**p)*col1)\n",
        "        denominator = np.sum(dummy[n]**p)\n",
        "        centroids[n,0] = numerator/denominator\n",
        "    #y values\n",
        "    for n in range(c):\n",
        "        numerator = np.sum((dummy[n]**p)*col2)\n",
        "        denominator = np.sum(dummy[n]**p)\n",
        "        centroids[n,1] = numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD4LWUSqQpAr"
      },
      "source": [
        "# update the fuzzy-pseudo partition\n",
        "def fp_partition():\n",
        "    dummy = np.zeros((c,size))\n",
        "    for n in range(c):\n",
        "        distance = (((col1-centroids[n,0])**2)+((col2-centroids[n,1])**2))**0.5\n",
        "        dummy[n] = (distance**-1)**(1/(p-1))\n",
        "    for m in range(c):\n",
        "        for n in range(size):       \n",
        "            numerator = dummy[m,n]\n",
        "            denominator = np.sum(dummy.transpose()[n])\n",
        "            w[n,m] = numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpRfjBklQpAr"
      },
      "source": [
        "# setup\n",
        "size = df_encoded.shape[0]\n",
        "col1 = df_pca[\"PC1\"].to_numpy()\n",
        "col2 = df_pca[\"PC2\"].to_numpy()\n",
        "c = 4 #number of clusters\n",
        "p = 1.8\n",
        "w = np.random.dirichlet(np.ones(c),size=size)\n",
        "centroids = np.zeros((c,2))\n",
        "compute_centroids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQfLZ92BQpAr"
      },
      "source": [
        "# repeat until w doesn't change\n",
        "for n in range(200):\n",
        "    fp_partition()\n",
        "    compute_centroids()\n",
        "#sum_squared_error()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3_QU5i6QpAr"
      },
      "source": [
        "# assign each data point to the cluster with most membership\n",
        "cluster = np.argmax(w, axis=1)\n",
        "df_pca_fuzzyc = df_pca.copy()\n",
        "new_data1 = df_pca_fuzzyc.assign(Clusters = cluster)\n",
        "df_final_fuzzyc = df_encoded.copy()\n",
        "new_data2 = df_final_fuzzyc.assign(Clusters = cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "bZ5jMQ7kQpAr"
      },
      "source": [
        "# plotting processed points\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
        "color = ['tab:blue', 'tab:orange', 'tab:red', 'tab:cyan', 'tab:green', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:olive', 'tab:gray']\n",
        "for n in range(c):\n",
        "    dummy = new_data1.query('Clusters==%d'%(n))\n",
        "    x = dummy[\"PC1\"].to_numpy()\n",
        "    y = dummy[\"PC2\"].to_numpy()\n",
        "    ax.scatter(x, y, c=color[n], s=5)\n",
        "    confidence_ellipse(x, y, ax, edgecolor=color[n] ,ls='--' ,lw=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx0tx0HQQpAs"
      },
      "source": [
        "# plotting countplot of clusters\n",
        "pl = sns.countplot(x=new_data1[\"Clusters\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coytgYsJQpAs"
      },
      "source": [
        "# plotting data points with color indicating the membership of each point\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
        "color = ['tab:blue', 'tab:orange', 'tab:red', 'tab:cyan', 'tab:green', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:olive', 'tab:gray']\n",
        "for n in range(c):\n",
        "    dummy = new_data2.query('Clusters==%d'%(n))\n",
        "    x = dummy[\"Spent\"].to_numpy()\n",
        "    y = dummy[\"Income\"].to_numpy()\n",
        "    ax.scatter(x, y, c=color[n], s=5)\n",
        "    confidence_ellipse(x, y, ax, edgecolor=color[n] ,ls='--' ,lw=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqAeBOSaQpAs"
      },
      "source": [
        "# graph different c value with increment=1\n",
        "fig, axes = plt.subplots(3,3,figsize=(10,10))\n",
        "axes = axes.reshape(-1)\n",
        "size = df_encoded.shape[0]\n",
        "col1 = df_pca[\"PC1\"].to_numpy()\n",
        "col2 = df_pca[\"PC2\"].to_numpy()\n",
        "color = ['tab:blue', 'tab:orange', 'tab:red', 'tab:cyan', 'tab:green', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:olive', 'tab:gray']\n",
        "c = 2\n",
        "for ax in axes:\n",
        "    p = 1.5\n",
        "    w = np.random.dirichlet(np.ones(c),size=size)\n",
        "    centroids = np.zeros((c,2))\n",
        "    compute_centroids()\n",
        "    for n in range(200):\n",
        "        fp_partition()\n",
        "        compute_centroids()\n",
        "    cluster = np.argmax(w, axis=1)\n",
        "    new_data = df_pca_fuzzyc.assign(Clusters = cluster)\n",
        "    for n in range(c):\n",
        "        dummy = new_data.query('Clusters==%d'%(n))\n",
        "        x = dummy[\"PC1\"].to_numpy()\n",
        "        y = dummy[\"PC2\"].to_numpy()\n",
        "        ax.scatter(x, y, c=color[n], s=1)\n",
        "        confidence_ellipse(x, y, ax, edgecolor=color[n] ,ls='--' ,lw=1)\n",
        "    ax.title.set_text(\"c = %d\"%(c))\n",
        "    c = c+1 #increment\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWwjn67JQpAs"
      },
      "source": [
        "# graph different p value with increment=0.1\n",
        "fig, axes = plt.subplots(4,4,figsize=(10,10),frameon=True)\n",
        "axes = axes.reshape(-1)\n",
        "size = df_encoded.shape[0]\n",
        "col1 = df_pca[\"PC1\"].to_numpy()\n",
        "col2 = df_pca[\"PC2\"].to_numpy()\n",
        "p = 1.1\n",
        "color = ['tab:blue', 'tab:orange', 'tab:red', 'tab:cyan', 'tab:green', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:olive', 'tab:gray']\n",
        "for ax in axes:\n",
        "    c = 4\n",
        "    w = np.random.dirichlet(np.ones(c),size=size)\n",
        "    centroids = np.zeros((c,2))\n",
        "    compute_centroids()\n",
        "    for n in range(200):\n",
        "        fp_partition()\n",
        "        compute_centroids()\n",
        "    cluster = np.argmax(w, axis=1)\n",
        "    new_data = df_pca_fuzzyc.assign(Clusters = cluster)\n",
        "    for n in range(c):\n",
        "        dummy = new_data.query('Clusters==%d'%(n))\n",
        "        x = dummy[\"PC1\"].to_numpy()\n",
        "        y = dummy[\"PC2\"].to_numpy()\n",
        "        ax.scatter(x, y, c=color[n], s=1)\n",
        "        confidence_ellipse(x, y, ax, edgecolor=color[n] ,ls='--' ,lw=1)\n",
        "    ax.title.set_text(\"p = %.1f\"%(p))\n",
        "    p = p+0.1 #increment\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}